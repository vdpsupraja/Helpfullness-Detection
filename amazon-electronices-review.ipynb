{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11304694,"sourceType":"datasetVersion","datasetId":7069773}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Import necessary libraries\nimport pandas as pd\nimport numpy as np\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import BertTokenizer, BertForSequenceClassification, AdamW\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, classification_report\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T05:17:18.397337Z","iopub.execute_input":"2025-04-07T05:17:18.397688Z","iopub.status.idle":"2025-04-07T05:17:18.402103Z","shell.execute_reply.started":"2025-04-07T05:17:18.397660Z","shell.execute_reply":"2025-04-07T05:17:18.401056Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"# Set random seed for reproducibility\ntorch.manual_seed(42)\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T05:17:20.516355Z","iopub.execute_input":"2025-04-07T05:17:20.516676Z","iopub.status.idle":"2025-04-07T05:17:20.522932Z","shell.execute_reply.started":"2025-04-07T05:17:20.516650Z","shell.execute_reply":"2025-04-07T05:17:20.521924Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"# 1. Load and preprocess the data\ndef load_and_preprocess_data():\n    # Assuming your data is in a CSV file\n    df = pd.read_csv('/kaggle/input/amazon-electronices/amazon_reviews.csv')  \n    \n    # Convert overall ratings to binary sentiment\n    # 4,5 = Positive (1), 1,2,3 = Negative (0)\n    df['sentiment'] = df['overall'].apply(lambda x: 1 if x >= 4 else 0)\n    \n    # Handle missing reviewText\n    df['reviewText'] = df['reviewText'].fillna('')\n    \n    return df[['reviewText', 'sentiment']]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T05:17:21.948198Z","iopub.execute_input":"2025-04-07T05:17:21.948485Z","iopub.status.idle":"2025-04-07T05:17:21.952866Z","shell.execute_reply.started":"2025-04-07T05:17:21.948463Z","shell.execute_reply":"2025-04-07T05:17:21.951913Z"}},"outputs":[],"execution_count":34},{"cell_type":"code","source":"# 2. Custom Dataset\nclass ReviewDataset(Dataset):\n    def __init__(self, texts, labels, tokenizer, max_len=128):\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        text = str(self.texts[idx])\n        label = self.labels[idx]\n        encoding = self.tokenizer.encode_plus(\n            text,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            padding='max_length',\n            truncation=True,\n            return_attention_mask=True,\n            return_tensors='pt'\n        )\n        return {\n            'input_ids': encoding['input_ids'].squeeze(),\n            'attention_mask': encoding['attention_mask'].squeeze(),\n            'labels': torch.tensor(label, dtype=torch.long)\n        }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T05:17:24.623394Z","iopub.execute_input":"2025-04-07T05:17:24.623730Z","iopub.status.idle":"2025-04-07T05:17:24.629055Z","shell.execute_reply.started":"2025-04-07T05:17:24.623701Z","shell.execute_reply":"2025-04-07T05:17:24.628122Z"}},"outputs":[],"execution_count":35},{"cell_type":"code","source":"# 3. Load data\ndf = load_and_preprocess_data()\ntrain_texts, val_texts, train_labels, val_labels = train_test_split(\n    df['reviewText'].values,\n    df['sentiment'].values,\n    test_size=0.2,\n    random_state=42\n)\n\n# 4. Tokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\n# 5. Dataset and DataLoader\ntrain_dataset = ReviewDataset(train_texts, train_labels, tokenizer)\nval_dataset = ReviewDataset(val_texts, val_labels, tokenizer)\n\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=16)\n\n# 6. Model\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\nmodel.to(device)\n\n# 7. Optimizer\noptimizer = AdamW(model.parameters(), lr=2e-5)\n\n# 8. Training loop\nepochs = 10\ntrain_loss_values, val_loss_values = [], []\n\nfor epoch in range(epochs):\n    model.train()\n    total_loss = 0\n    loop = tqdm(train_loader, desc=f'Epoch {epoch+1}')\n    for batch in loop:\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['labels'].to(device)\n\n        model.zero_grad()\n        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n        loss = outputs.loss\n        total_loss += loss.item()\n\n        loss.backward()\n        optimizer.step()\n\n        loop.set_postfix(loss=loss.item())\n    avg_train_loss = total_loss / len(train_loader)\n    train_loss_values.append(avg_train_loss)\n\n    # Validation\n    model.eval()\n    val_loss = 0\n    predictions, true_labels = [], []\n    with torch.no_grad():\n        for batch in val_loader:\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            labels = batch['labels'].to(device)\n\n            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n            loss = outputs.loss\n            val_loss += loss.item()\n\n            logits = outputs.logits\n            preds = torch.argmax(logits, dim=1).flatten()\n            predictions.extend(preds.cpu().numpy())\n            true_labels.extend(labels.cpu().numpy())\n\n    avg_val_loss = val_loss / len(val_loader)\n    val_loss_values.append(avg_val_loss)\n\n    acc = accuracy_score(true_labels, predictions)\n    print(f'Validation Loss: {avg_val_loss:.4f} | Accuracy: {acc:.4f}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T05:19:37.200449Z","iopub.execute_input":"2025-04-07T05:19:37.200786Z","iopub.status.idle":"2025-04-07T05:23:58.128481Z","shell.execute_reply.started":"2025-04-07T05:19:37.200758Z","shell.execute_reply":"2025-04-07T05:23:58.127585Z"}},"outputs":[{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\nEpoch 1: 100%|██████████| 100/100 [00:23<00:00,  4.23it/s, loss=0.254]\n","output_type":"stream"},{"name":"stdout","text":"Validation Loss: 0.3127 | Accuracy: 0.8425\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2: 100%|██████████| 100/100 [00:23<00:00,  4.25it/s, loss=0.0711]\n","output_type":"stream"},{"name":"stdout","text":"Validation Loss: 0.2933 | Accuracy: 0.8750\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3: 100%|██████████| 100/100 [00:23<00:00,  4.25it/s, loss=0.038] \n","output_type":"stream"},{"name":"stdout","text":"Validation Loss: 0.3276 | Accuracy: 0.8800\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4: 100%|██████████| 100/100 [00:23<00:00,  4.25it/s, loss=0.101] \n","output_type":"stream"},{"name":"stdout","text":"Validation Loss: 0.4897 | Accuracy: 0.8600\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5: 100%|██████████| 100/100 [00:23<00:00,  4.25it/s, loss=0.0135]\n","output_type":"stream"},{"name":"stdout","text":"Validation Loss: 0.5524 | Accuracy: 0.8675\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6: 100%|██████████| 100/100 [00:23<00:00,  4.25it/s, loss=0.0308] \n","output_type":"stream"},{"name":"stdout","text":"Validation Loss: 0.5711 | Accuracy: 0.8775\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7: 100%|██████████| 100/100 [00:23<00:00,  4.24it/s, loss=0.000701]\n","output_type":"stream"},{"name":"stdout","text":"Validation Loss: 0.7065 | Accuracy: 0.8725\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8: 100%|██████████| 100/100 [00:23<00:00,  4.25it/s, loss=0.000973]\n","output_type":"stream"},{"name":"stdout","text":"Validation Loss: 0.6345 | Accuracy: 0.8850\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9: 100%|██████████| 100/100 [00:23<00:00,  4.25it/s, loss=0.0019] \n","output_type":"stream"},{"name":"stdout","text":"Validation Loss: 0.5821 | Accuracy: 0.8850\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10: 100%|██████████| 100/100 [00:23<00:00,  4.25it/s, loss=0.00263]\n","output_type":"stream"},{"name":"stdout","text":"Validation Loss: 0.6301 | Accuracy: 0.8700\n","output_type":"stream"}],"execution_count":37},{"cell_type":"code","source":"# 9. Classification report\nprint(\"Classification Report:\")\nprint(classification_report(true_labels, predictions))\n\n# 10. Save model\nmodel.save_pretrained('bert_sentiment_model')\ntokenizer.save_pretrained('bert_sentiment_model')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T05:24:58.418187Z","iopub.execute_input":"2025-04-07T05:24:58.418517Z","iopub.status.idle":"2025-04-07T05:24:59.511641Z","shell.execute_reply.started":"2025-04-07T05:24:58.418490Z","shell.execute_reply":"2025-04-07T05:24:59.510907Z"}},"outputs":[{"name":"stdout","text":"Classification Report:\n              precision    recall  f1-score   support\n\n           0       0.69      0.66      0.68        82\n           1       0.91      0.92      0.92       318\n\n    accuracy                           0.87       400\n   macro avg       0.80      0.79      0.80       400\nweighted avg       0.87      0.87      0.87       400\n\n","output_type":"stream"},{"execution_count":38,"output_type":"execute_result","data":{"text/plain":"('bert_sentiment_model/tokenizer_config.json',\n 'bert_sentiment_model/special_tokens_map.json',\n 'bert_sentiment_model/vocab.txt',\n 'bert_sentiment_model/added_tokens.json')"},"metadata":{}}],"execution_count":38},{"cell_type":"code","source":"\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}